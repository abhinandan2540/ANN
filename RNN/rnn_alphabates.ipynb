{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc65bf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# W: hidden to hidden weights\n",
    "# U: input to hidden weights\n",
    "# b: hidden bias\n",
    "# V: hidden to output weights\n",
    "# C: output bias\n",
    "\n",
    "input_size=26\n",
    "hidden_size=10\n",
    "output_size=26\n",
    "\n",
    "\n",
    "# parameters\n",
    "# hidden hidden\n",
    "W=np.random.randn(hidden_size, hidden_size)*0.01\n",
    "\n",
    "# input hidden\n",
    "U=np.random.randn(hidden_size, input_size)*0.01 \n",
    "\n",
    "# hidden output\n",
    "V=np.random.randn(output_size, hidden_size)*0.01 \n",
    "\n",
    "\n",
    "b=np.zeros((hidden_size, 1)) # (10,1)\n",
    "c=np.zeros((output_size, 1)) # (5,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4462a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation function\n",
    "\n",
    "def tanh(s):\n",
    "    return np.tanh(s)\n",
    "\n",
    "def derivative_tanh(s):\n",
    "    return 1-np.tanh(s)**2\n",
    "\n",
    "def softmax(o):\n",
    "    e = np.exp(o - np.max(o))\n",
    "    return e / np.sum(e, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c862421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "# cross entropy loss function\n",
    "# L= - np.sum(y_true * log(y_pred))\n",
    "\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    eps=1e-15\n",
    "    loss=0.0\n",
    "    for t in range(len(y_true)):\n",
    "        loss-=np.sum(y_true[t]*np.log(y_pred[t]+eps))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "109c85a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sequential alphabates\n",
    "alphabates='ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "vocab_list=list(alphabates)\n",
    "vocab_size=26 # 26 vocabs are present \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2d97fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating mappings\n",
    "char_to_index={ch:i for i,ch in enumerate(vocab_list)}\n",
    "index_to_char={i:ch for i,ch in enumerate(vocab_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6c19dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25}\n",
      "{0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n"
     ]
    }
   ],
   "source": [
    "print(char_to_index)\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eace0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoded(sequence, char_to_index, vocab_size):\n",
    "    encoded=np.zeros((len(sequence), vocab_size))\n",
    "    for i, ch in enumerate(sequence):\n",
    "        encoded[i, char_to_index[ch]]=1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb40bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26, 26, 1)\n",
      "(26, 26, 1)\n"
     ]
    }
   ],
   "source": [
    "X=one_hot_encoded(alphabates, char_to_index=char_to_index, vocab_size=26)\n",
    "\n",
    "# X is the input (that we'll use to train RNN)\n",
    "\n",
    "# y_true : that we want to get from RNN\n",
    "# we actually want to predict same alphabates again from RNN, after updating weights, and biases after training\n",
    "# in testing\n",
    "\n",
    "y_true=one_hot_encoded(alphabates, char_to_index=char_to_index, vocab_size=26)\n",
    "\n",
    "X=X.reshape(26,26,1)\n",
    "y_true=y_true.reshape(26,26,1)\n",
    "\n",
    "print(X.shape)\n",
    "print(y_true.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb3e03ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(26, 10)\n",
      "(10, 26)\n",
      "(10, 1)\n",
      "(26, 1)\n"
     ]
    }
   ],
   "source": [
    "# shapes\n",
    "\n",
    "print(W.shape) # hidden hidden\n",
    "print(V.shape) # hidden output\n",
    "print(U.shape) # input hidden\n",
    "print(b.shape) # hidden\n",
    "print(c.shape) # output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e816eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "\n",
    "def forward_pass(W, U, V, b, c, a_prev, X):\n",
    "    s,a,o,y_pred = {},{},{},{}\n",
    "    a[-1]=a_prev # a[t-1] # must be (10,1)\n",
    "\n",
    "    for t in range(len(X)):\n",
    "        s[t]=W@a[t-1]+U@X[t]+b # (10,1)\n",
    "        a[t]=tanh(s[t]) # (10,1)\n",
    "        o[t]=V@a[t]+c # (26,1)\n",
    "        y_pred[t]=softmax(o[t]) # (26,1)\n",
    "\n",
    "    return s,a,o,y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0daf7eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation through time\n",
    "\n",
    "def BPTT(y_pred, y_true, X, a, c, V, U, W, b, s):\n",
    "    dL_dc=np.zeros_like(c)\n",
    "    dL_dv=np.zeros_like(V)\n",
    "    dL_dU=np.zeros_like(U)\n",
    "    dL_dW=np.zeros_like(W)\n",
    "    dL_db=np.zeros_like(b)\n",
    "\n",
    "    dL_da_next=np.zeros_like(a[0])\n",
    "    for t in reversed(range(len(X))):\n",
    "        \n",
    "        # output gradients\n",
    "        dL_do=y_pred[t]-y_true[t] # (26,1)\n",
    "        dL_dv+=dL_do@a[t].T # (26,10)\n",
    "        dL_dc+=dL_do # (26,1)\n",
    "\n",
    "        # hidden gradients\n",
    "        dL_da=V.T@dL_do+dL_da_next # (10,1)\n",
    "        dL_ds=dL_da*derivative_tanh(s[t]) # (10,1)\n",
    "\n",
    "        dL_db+=dL_ds # (10,1)\n",
    "        dL_dW+=dL_ds@a[t-1].T # (10,10)\n",
    "        dL_dU+=dL_ds@X[t].T # (10,26)\n",
    "\n",
    "        dL_da_next = W.T@dL_ds\n",
    "\n",
    "    return dL_dc, dL_dv, dL_dU,dL_dW, dL_db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c5a45ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0| Train Loss:0.06582958641322756| Test Loss:0.06582259540953755\n",
      "Epoch:1000| Train Loss:0.05950734718723969| Test Loss:0.059501630969344375\n",
      "Epoch:2000| Train Loss:0.05429033679344231| Test Loss:0.054285576318196874\n",
      "Epoch:3000| Train Loss:0.049912299783700584| Test Loss:0.049908274172990395\n",
      "Epoch:4000| Train Loss:0.04618610402625759| Test Loss:0.04618265553537003\n",
      "Epoch:5000| Train Loss:0.04297639117110483| Test Loss:0.0429734041510467\n",
      "Epoch:6000| Train Loss:0.04018283526867072| Test Loss:0.04018022301087464\n",
      "Epoch:7000| Train Loss:0.03772950149031452| Test Loss:0.037727197712136284\n",
      "Epoch:8000| Train Loss:0.035557860690425484| Test Loss:0.03555581385828125\n",
      "Epoch:9000| Train Loss:0.03362207410092296| Test Loss:0.03362024354622393\n",
      "Epoch:10000| Train Loss:0.03188573180885569| Test Loss:0.03188408500943955\n",
      "Epoch:11000| Train Loss:0.0303195477328018| Test Loss:0.03031805837142348\n",
      "Epoch:12000| Train Loss:0.02889969913584316| Test Loss:0.028898345688441525\n",
      "Epoch:13000| Train Loss:0.02760660980429014| Test Loss:0.027605374496795475\n",
      "Epoch:14000| Train Loss:0.02642404450851904| Test Loss:0.026422912534630624\n",
      "Epoch:15000| Train Loss:0.025338425655967847| Test Loss:0.025337384582836686\n",
      "Epoch:16000| Train Loss:0.024338311038883346| Test Loss:0.02433735035006707\n",
      "Epoch:17000| Train Loss:0.023413990051888947| Test Loss:0.023413100791959636\n",
      "Epoch:18000| Train Loss:0.02255716817308103| Test Loss:0.02255634266847435\n",
      "Epoch:19000| Train Loss:0.021760717994017333| Test Loss:0.021759949631693944\n",
      "Epoch:20000| Train Loss:0.0210184809814058| Test Loss:0.02101776403183806\n",
      "Epoch:21000| Train Loss:0.020325108308198322| Test Loss:0.020324437782649\n",
      "Epoch:22000| Train Loss:0.019675932057900965| Test Loss:0.019675303592485288\n",
      "Epoch:23000| Train Loss:0.019066860249720975| Test Loss:0.019066270010598532\n",
      "Epoch:24000| Train Loss:0.018494290699086736| Test Loss:0.018493735304377604\n",
      "Epoch:25000| Train Loss:0.017955039885799984| Test Loss:0.01795451634092042\n",
      "Epoch:26000| Train Loss:0.017446283865932402| Test Loss:0.017445789509717518\n",
      "Epoch:27000| Train Loss:0.016965508914057004| Test Loss:0.01696504137362561\n",
      "Epoch:28000| Train Loss:0.016510470076658033| Test Loss:0.016510027229424906\n",
      "Epoch:29000| Train Loss:0.016079156196092584| Test Loss:0.016078736137618323\n",
      "Epoch:30000| Train Loss:0.015669760256669704| Test Loss:0.015669361273354845\n",
      "Epoch:31000| Train Loss:0.015280654131515463| Test Loss:0.015280274677288954\n",
      "Epoch:32000| Train Loss:0.014910366986840737| Test Loss:0.014910005663172292\n",
      "Epoch:33000| Train Loss:0.014557566740255106| Test Loss:0.014557222278979053\n",
      "Epoch:34000| Train Loss:0.014221044080950465| Test Loss:0.014220715329429747\n",
      "Epoch:35000| Train Loss:0.013899698648145092| Test Loss:0.013899384556406262\n",
      "Epoch:36000| Train Loss:0.013592527035242555| Test Loss:0.013592226644799518\n",
      "Epoch:37000| Train Loss:0.013298612344471985| Test Loss:0.013298324778577454\n",
      "Epoch:38000| Train Loss:0.013017115063153652| Test Loss:0.01301683951827623\n",
      "Epoch:39000| Train Loss:0.012747265070583285| Test Loss:0.012747000808909382\n",
      "Epoch:40000| Train Loss:0.01248835461537541| Test Loss:0.012488100958243863\n",
      "Epoch:41000| Train Loss:0.012239732128593106| Test Loss:0.012239488450672948\n",
      "Epoch:42000| Train Loss:0.0120007967588542| Test Loss:0.012000562483017453\n",
      "Epoch:43000| Train Loss:0.011770993533035363| Test Loss:0.011770768125793009\n",
      "Epoch:44000| Train Loss:0.01154980906058637| Test Loss:0.011549592028045804\n",
      "Epoch:45000| Train Loss:0.011336767711541544| Test Loss:0.0113365585958052\n",
      "Epoch:46000| Train Loss:0.011131428208391407| Test Loss:0.01113122658433028\n",
      "Epoch:47000| Train Loss:0.01093338058046211| Test Loss:0.010933186052841515\n",
      "Epoch:48000| Train Loss:0.010742243436609348| Test Loss:0.010742055637500298\n",
      "Epoch:49000| Train Loss:0.010557661518095162| Test Loss:0.010557480104566152\n",
      "Epoch:50000| Train Loss:0.010379303498611676| Test Loss:0.010379128150637615\n",
      "Epoch:51000| Train Loss:0.01020686000285816| Test Loss:0.010206690421434692\n",
      "Epoch:52000| Train Loss:0.010040041818697772| Test Loss:0.01003987772415496\n",
      "Epoch:53000| Train Loss:0.0098785782812487| Test Loss:0.009878419411702668\n",
      "Epoch:54000| Train Loss:0.009722215809888381| Test Loss:0.009722061919854735\n",
      "Epoch:55000| Train Loss:0.009570716581549807| Test Loss:0.009570567440683695\n",
      "Epoch:56000| Train Loss:0.00942385732577346| Test Loss:0.009423712717710898\n",
      "Epoch:57000| Train Loss:0.009281428228595465| Test Loss:0.009281287949917876\n",
      "Epoch:58000| Train Loss:0.009143231934061065| Test Loss:0.009143095793336926\n",
      "Epoch:59000| Train Loss:0.009009082633312375| Test Loss:0.009008950450225504\n",
      "Epoch:60000| Train Loss:0.00887880523242585| Test Loss:0.008878676836988047\n",
      "Epoch:61000| Train Loss:0.008752234591207485| Test Loss:0.008752109823020977\n",
      "Epoch:62000| Train Loss:0.008629214825936888| Test Loss:0.00862909353353283\n",
      "Epoch:63000| Train Loss:0.008509598669915252| Test Loss:0.008509480710142642\n",
      "Epoch:64000| Train Loss:0.008393246886290149| Test Loss:0.008393132123744183\n",
      "Epoch:65000| Train Loss:0.008280027728235445| Test Loss:0.008279916034751023\n",
      "Epoch:66000| Train Loss:0.00816981644209925| Test Loss:0.00816970769626445\n",
      "Epoch:67000| Train Loss:0.008062494809577707| Test Loss:0.008062388896304465\n",
      "Epoch:68000| Train Loss:0.007957950725387538| Test Loss:0.007957847535495572\n",
      "Epoch:69000| Train Loss:0.007856077807262463| Test Loss:0.00785597723710943\n",
      "Epoch:70000| Train Loss:0.007756775035420121| Test Loss:0.007756676986557996\n",
      "Epoch:71000| Train Loss:0.007659946418961223| Test Loss:0.007659850797812375\n",
      "Epoch:72000| Train Loss:0.007565500686844386| Test Loss:0.007565407404398529\n",
      "Epoch:73000| Train Loss:0.007473351001364891| Test Loss:0.007473259972919488\n",
      "Epoch:74000| Train Loss:0.007383414692285753| Test Loss:0.007383325837166332\n",
      "Epoch:75000| Train Loss:0.007295613009818574| Test Loss:0.007295526251160581\n",
      "Epoch:76000| Train Loss:0.007209870895016213| Test Loss:0.0072097861595382135\n",
      "Epoch:77000| Train Loss:0.007126116766084021| Test Loss:0.007126033983874579\n",
      "Epoch:78000| Train Loss:0.007044282319374499| Test Loss:0.007044201423707602\n",
      "Epoch:79000| Train Loss:0.006964302343888151| Test Loss:0.006964223271045966\n",
      "Epoch:80000| Train Loss:0.006886114548242821| Test Loss:0.00688603723733827\n",
      "Epoch:81000| Train Loss:0.0068096593991073115| Test Loss:0.006809583791932035\n",
      "Epoch:82000| Train Loss:0.006734879970253058| Test Loss:0.006734806011142866\n",
      "Epoch:83000| Train Loss:0.006661721801425973| Test Loss:0.006661649437109625\n",
      "Epoch:84000| Train Loss:0.006590132766267899| Test Loss:0.0065900619457408705\n",
      "Epoch:85000| Train Loss:0.006520062948633869| Test Loss:0.006519993623052016\n",
      "Epoch:86000| Train Loss:0.006451464526719509| Test Loss:0.00645139664926808\n",
      "Epoch:87000| Train Loss:0.0063842916643593916| Test Loss:0.0063842251901642414\n",
      "Epoch:88000| Train Loss:0.006318500409058201| Test Loss:0.006318435295076794\n",
      "Epoch:89000| Train Loss:0.00625404859622761| Test Loss:0.006253984801159573\n",
      "Epoch:90000| Train Loss:0.006190895759215856| Test Loss:0.006190833243416223\n",
      "Epoch:91000| Train Loss:0.006129003044688188| Test Loss:0.006128941770081712\n",
      "Epoch:92000| Train Loss:0.006068333133055693| Test Loss:0.0060682730630659725\n",
      "Epoch:93000| Train Loss:0.006008850163533623| Test Loss:0.006008791263009221\n",
      "Epoch:94000| Train Loss:0.005950519663581003| Test Loss:0.005950461898718886\n",
      "Epoch:95000| Train Loss:0.00589330848235804| Test Loss:0.005893251820651704\n",
      "Epoch:96000| Train Loss:0.005837184728020484| Test Loss:0.005837129138182178\n",
      "Epoch:97000| Train Loss:0.005782117708503049| Test Loss:0.005782063160422664\n",
      "Epoch:98000| Train Loss:0.005728077875647045| Test Loss:0.005728024340333013\n",
      "Epoch:99000| Train Loss:0.0056750367724115285| Test Loss:0.005674984221934716\n"
     ]
    }
   ],
   "source": [
    "# let's train this model\n",
    "\n",
    "epochs=100000\n",
    "lr=0.01\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # training loop\n",
    "    a_prev = np.zeros((hidden_size, 1))\n",
    "    s, a, o, y_pred_train = forward_pass(W=W, U=U, V=V, b=b, c=c, a_prev=a_prev, X=X) \n",
    "    L_train = cross_entropy_loss(y_pred=y_pred_train, y_true=y_true) \n",
    "    dL_dc, dL_dV, dL_dU, dL_dW, dL_db = BPTT(y_pred=y_pred_train,\n",
    "                                             y_true=y_true,\n",
    "                                             X=X,\n",
    "                                             a=a,\n",
    "                                             c=c,\n",
    "                                             V=V,\n",
    "                                             U=U,\n",
    "                                             W=W,\n",
    "                                             b=b,\n",
    "                                             s=s\n",
    "                                             )\n",
    "    \n",
    "    V-=lr*dL_dV\n",
    "    c-=lr*dL_dc\n",
    "    U-=lr*dL_dU\n",
    "    W-=lr*dL_dW\n",
    "    b-=lr*dL_db\n",
    "\n",
    "    # testing loop\n",
    "    s, a, o, y_pred_test = forward_pass(\n",
    "        W=W, U=U, V=V, b=b, c=c, a_prev=a_prev, X=X)\n",
    "    L_test = cross_entropy_loss(y_pred=y_pred_test, y_true=y_true)\n",
    "   \n",
    "\n",
    "    if (epoch%1000)==0:\n",
    "        print(f'Epoch:{epoch}| Train Loss:{L_train}| Test Loss:{L_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b1dc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction my RNN, also known as sequence generation (inference)\n",
    "# basic principle of sequence generation is\n",
    "# 1. feed the initial seed (eg: ABCDE)\n",
    "# 2. use the last predicted character as the next input\n",
    "# 3. repeat\n",
    "\n",
    "# whole process is called autoregressive generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc892d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one_hot vector\n",
    "\n",
    "def one_hot(idx, vocab_size=26):\n",
    "    v = np.zeros((vocab_size, 1))\n",
    "    v[idx, 0] = 1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94d280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def autoregressive_generation(seed,U,W,V,b,c,char_to_index, index_to_char,length=20):\n",
    "    hidden_size=W.shape[0]\n",
    "    a_prev=np.zeros((hidden_size,1))\n",
    "\n",
    "    for ch in seed:\n",
    "        x = one_hot(char_to_index[ch])\n",
    "        s = W @ a_prev + U @ x + b\n",
    "        a_prev = np.tanh(s)\n",
    "    \n",
    "    result=seed\n",
    "\n",
    "    current_char = seed[-1]\n",
    "\n",
    "    for _ in range(length):\n",
    "        x = one_hot(char_to_index[current_char])\n",
    "\n",
    "        s = W @ a_prev + U @ x + b\n",
    "        a = np.tanh(s)\n",
    "\n",
    "        o = V @ a + c\n",
    "        y_pred = softmax(o)\n",
    "\n",
    "        next_idx = int(np.argmax(y_pred))\n",
    "        next_char = index_to_char[next_idx]\n",
    "        result += next_char\n",
    "\n",
    "        # update\n",
    "        a_prev = a\n",
    "        current_char = next_char\n",
    "\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e918e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGHIJK\n"
     ]
    }
   ],
   "source": [
    "seed='ABCDEFG'\n",
    "\n",
    "print(autoregressive_generation(seed=seed,\n",
    "                                U=U,\n",
    "                                W=W,\n",
    "                                V=V,\n",
    "                                b=b,\n",
    "                                c=c,\n",
    "                                char_to_index=char_to_index,\n",
    "                                index_to_char=index_to_char,\n",
    "                                length=4\n",
    "                                ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e1f7c1e",
   "metadata": {},
   "source": [
    "*Building MultiLayer Perceptron*\n",
    "\n",
    "`1 input node`, `1 hidden layer (10 neurons)`, `1 output node`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d61f3637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "X=np.random.rand(1).reshape(1,1) # 1 input node value\n",
    "Y_true=1 # True value\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a93045eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02509198  0.09014286  0.04639879  0.0197317  -0.06879627 -0.0688011\n",
      "  -0.08838328  0.07323523  0.020223    0.04161452]]\n",
      "[[-0.0958831   0.09398197  0.06648853 -0.05753218 -0.06363501 -0.0633191\n",
      "  -0.03915155  0.00495129 -0.013611   -0.04175417]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "num_inputs=1 # number of inputs 1 (x)\n",
    "num_neurons=10 # number of hidden layer nodes (10)\n",
    "\n",
    "# WEIGHTS\n",
    "# INPUT LAYER -> HIDDEN LAYER\n",
    "weight_input_hidden=np.random.uniform(low=-0.1, high=0.1, size=(num_inputs, num_neurons))\n",
    "\n",
    "# BIAS (APPLIED IN HIDDEN LAYER)\n",
    "bias_hidden=np.random.uniform(low=-0.1, high=0.1, size=(1, num_neurons))\n",
    "\n",
    "print(weight_input_hidden)\n",
    "print(bias_hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72e7d170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1)\n",
      "(1, 10)\n",
      "(1, 10)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(weight_input_hidden.shape)\n",
    "print(bias_hidden.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "168b8fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll be using sigmoid activation function\n",
    "def sigmoid_activation_func(z):\n",
    "    return 1/(1+np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e6491f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WEIGHT, BIAS FOR \n",
    "# HIDDEN LAYER -> OUTPUT LAYER\n",
    "\n",
    "weight_hidden_output=np.random.rand(1,10)\n",
    "bias_output=np.random.rand(1).reshape(1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "410efca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61185289 0.13949386 0.29214465 0.36636184 0.45606998 0.78517596\n",
      "  0.19967378 0.51423444 0.59241457 0.04645041]]\n",
      "[[-0.0958831   0.09398197  0.06648853 -0.05753218 -0.06363501 -0.0633191\n",
      "  -0.03915155  0.00495129 -0.013611   -0.04175417]]\n"
     ]
    }
   ],
   "source": [
    "print(weight_hidden_output)\n",
    "print(bias_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55e77f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 10)\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(weight_hidden_output.shape)\n",
    "print(bias_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "98523ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss : 0.0001, Y_pred : 0.9886\n",
      "Epoch 1000 | Loss : 0.0001, Y_pred : 0.9891\n",
      "Epoch 2000 | Loss : 0.0001, Y_pred : 0.9896\n",
      "Epoch 3000 | Loss : 0.0001, Y_pred : 0.9900\n",
      "Epoch 4000 | Loss : 0.0000, Y_pred : 0.9904\n",
      "Epoch 5000 | Loss : 0.0000, Y_pred : 0.9907\n",
      "Epoch 6000 | Loss : 0.0000, Y_pred : 0.9910\n",
      "Epoch 7000 | Loss : 0.0000, Y_pred : 0.9913\n",
      "Epoch 8000 | Loss : 0.0000, Y_pred : 0.9915\n",
      "Epoch 9000 | Loss : 0.0000, Y_pred : 0.9918\n"
     ]
    }
   ],
   "source": [
    "epochs=10000\n",
    "learning_rate=0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # FORWARD PASS\n",
    "    # INPUT LAYER -> HIDDEN LAYER\n",
    "    Z_hidden=X@weight_input_hidden+bias_hidden\n",
    "    A_hidden=sigmoid_activation_func(Z_hidden)\n",
    "\n",
    "    # HIDDEN LAYER -> OUTPUT LAYER\n",
    "    Z_output=A_hidden@weight_hidden_output.T+bias_output\n",
    "    Y_pred=sigmoid_activation_func(Z_output)\n",
    "\n",
    "    # loss (L) : sum of squared error\n",
    "    L=0.5*(Y_pred - Y_true)**2\n",
    "\n",
    "    # BACKPROPAGATION\n",
    "\n",
    "    # OUTPUT LAYER -> HIDDEN LAYER\n",
    "    # dL/dW = (dL/dY_pred) * (dY_pred/dZ) * (dZ/dW)\n",
    "    dL_dY_pred=(Y_pred - Y_true)\n",
    "    dY_pred_dZ=Y_pred*(1-Y_pred)\n",
    "    dZ_dW_hidden_output=A_hidden\n",
    "\n",
    "    # dL/dZ = (dL/dY_pred)*(dY_pred/dZ)\n",
    "    dL_dZ_output=dL_dY_pred*dY_pred_dZ\n",
    "\n",
    "    # dL/dW = (dL/dW)*(dZ/dW)\n",
    "    dL_dW_hidden_output=dL_dZ_output*dZ_dW_hidden_output\n",
    "\n",
    "    # dL/db = (dL/dY_pred)*(dY_pred/dZ)*(dZ/db)\n",
    "    # dL/dZ = (dL/dY_pred)*(dY_pred/dZ)\n",
    "    dZ_db_output=1\n",
    "    dL_db_output=dL_dZ_output*dZ_db_output\n",
    "\n",
    "    # UPDATING WEIGHTS, BIAS\n",
    "    weight_hidden_output-=learning_rate*dL_dW_hidden_output\n",
    "    bias_output-=learning_rate*dL_db_output\n",
    "\n",
    "    \n",
    "    # HIDDEN LAYER -> INPUT LAYER\n",
    "    # dL/dW = (dL/dA_hidden)*(dA_hidden/dZ)*(dZ/dW)\n",
    "    # dL/dA_hidden = (dL/dY_pred)*(dY_pred/dZ)*(dZ/dA_hidden)\n",
    "    # (dL/dY_pred)*(dY_pred/dZ) = (dL/dZ)\n",
    "    # dL/dA_hidden = (dL/dZ)*(dZ/dA_hidden)\n",
    "    \n",
    "    # dZ/dA_hidden\n",
    "    dZ_dA_hidden=weight_hidden_output\n",
    "\n",
    "    # dL/dA_hidden\n",
    "    dL_dA_hidden=dL_dZ_output*dZ_dA_hidden\n",
    "\n",
    "    # dA_hidden/dZ = A_hidden*(1-A_hidden)\n",
    "    dA_hidden_dZ=A_hidden*(1-A_hidden)\n",
    "\n",
    "    # dZ/dW\n",
    "    dZ_dW_input_hidden=X\n",
    "\n",
    "    # dL/dZ_hidden=(dL/dA_hidden)*(dA_hidden/dZ)\n",
    "    dL_dZ_hidden=dL_dA_hidden*dA_hidden_dZ\n",
    "\n",
    "    # dL/dW = (dL/dZ)*(dZ/dW)\n",
    "    dL_dW_input_hidden=dL_dZ_hidden*dZ_dW_input_hidden\n",
    "\n",
    "    # dL/db = (dL/dZ)*(dZ/db)\n",
    "    dZ_db_hidden=1\n",
    "    dL_db_hidden=dL_dZ_hidden*dZ_db_hidden\n",
    "\n",
    "    # WEIGHTS, BIAS UPDATION\n",
    "    weight_input_hidden-=learning_rate*dL_dW_input_hidden\n",
    "    bias_hidden-=learning_rate*dL_db_hidden\n",
    "\n",
    "\n",
    "    if epoch%1000==0:\n",
    "        loss_val=L.item() if np.ndim(L)==0 or L.size==1 else np.mean(L)\n",
    "        Y_pred_val=Y_pred.item() if np.ndim(Y_pred)==0 or Y_pred.size==1 else np.mean(Y_pred)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Loss : {loss_val:.4f}, Y_pred : {Y_pred_val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
